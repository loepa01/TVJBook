{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "20ab2f66",
   "metadata": {},
   "source": [
    "# Úvod do neuronových sítí"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5db5c4f3",
   "metadata": {},
   "source": [
    "Motivace - fungování mozku, propojení množství neuronů, aktivace $y$ (aktivační potenciál)\n",
    "\n",
    "$$\n",
    "y = f(\\sum_{i=1}^{N}{w_i x_i + b})\n",
    "$$\n",
    "\n",
    "![Neuron](../images/perceptron2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f94191bc",
   "metadata": {},
   "source": [
    "## Perceptron\n",
    "\n",
    "Demo: https://playground.tensorflow.org/\n",
    "\n",
    "Základní architektura neuronové sítě. Skládá se z několika vrstev neuronů - vstup, výstup + skryté vrstvy:\n",
    "\n",
    "V sousedních dvou vrstvách je každý neuron z jedné vrstvy propojen s každým z druhé vrstvy.\n",
    "\n",
    "<img src=\"../images/perceptron3.png\" alt=\"Perceptron\" width=\"800\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c73f77b9",
   "metadata": {},
   "source": [
    "Trénování jednoduchého perceptronu:\n",
    "\n",
    "<img src=\"../images/perceptron_uceni.png\" alt=\"Perceptron\" width=\"1200\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "423d0f20",
   "metadata": {},
   "source": [
    "Trénování hlubokých neuronových sítí:\n",
    "\n",
    "* Pomocí optimalizačních metod + Zpětné šíření změny (*Backpropagation*) skrz neuronovou síť:\n",
    "    - Metoda stochastického nejvetšího spádu\n",
    "    - Adam, ...\n",
    "* Učení s učitelem vs bez učitele\n",
    "\n",
    "\n",
    "<img src=\"../images/backpropagation.png\" alt=\"Backpropagation\" width=\"800\"/>\n",
    "\n",
    "Chyba na výstupu se dá vizualizovat jako křivka/funkce, u které hledáme minimum - model se dopustí nejmenší chyby.\n",
    "\n",
    "<img src=\"../images/energy_surphase.png\" alt=\"Backpropagation\" width=\"600\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ef7458b",
   "metadata": {},
   "source": [
    "Důležité parametry pro trénování neuronových sítí:\n",
    "* *learning rate* - rychlost učení, \"síla\" opravy chyby na výstupu při *backpropagaci*\n",
    "* *batch training* - současné vyhodnocení výstupu pro více vstupů najednou, urychlení trénování\n",
    "* *epochy* - kolikrát model \"viděl\" všechna data, často je potřeba několik epoch pro získání přesného modelu\n",
    "* *aktivační funkce* - lineární vs nelineární: sigmoid, tanh, ReLU\n",
    "\n",
    "<img src=\"../images/activacni_fce.png\" alt=\"Aktivační funkce\" width=\"1200\"/>\n",
    "\n",
    "Problém učení pro hluboké sítě:\n",
    "* problém s *explodujícími a mizejícími gradienty* - čím je neuronová síť hlubší, tím náročnější je trénování a pomalejší šíření změn zpět od výstupu\n",
    "\n",
    "Pokročilé techniky pro efektivní trénování hlubokých neuronových sítí:\n",
    "* *dropout* vrstva - při průchodu neuronovou sítí se ve zvolené vrstvě dočasně **vypne** zvolená část neuronů; umožnuje lépe prozkoumávat terén celkové chyby a zefektivnit učení\n",
    "* *batch normalisation*\n",
    "* ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87d77a50",
   "metadata": {},
   "source": [
    "## Konvoluční neuronové sítě\n",
    "\n",
    "Demo: https://cs.stanford.edu/people/karpathy/convnetjs/demo/mnist.html\n",
    "\n",
    "Motivace:\n",
    "* Vylepšení perceptronu\n",
    "* Zahrnutí základní vlastnosti obrázků přímo do architektury modelu - nezávislost posunutí objektu v obrázku (pořád stejný obraz)\n",
    "\n",
    "<img src=\"../images/convnet.jpg\" alt=\"Konvoluční síť\" width=\"1200\"/>\n",
    "\n",
    "Již poměrně hluboké sítě - potřeba aplikovat pokročilé techniky trénování.\n",
    "\n",
    "Parametry konvoluční sítě:\n",
    "* velikost okna (kernel size)\n",
    "* počet kanálů (channels) - určuje, kolik vlastností se v dané vstvě z obrázku extrahuje\n",
    "* *max-pooling* - speciální mezivrstva, která redukuje množství informace - průměrování okolních pixelů"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86b4e57f",
   "metadata": {},
   "source": [
    "## One-hot-encoding\n",
    "\n",
    "* *Label encoding*: \"red\" = 1, \"blue\" = 2, \"green\" = 3\n",
    "* *one-hot encoding*: \"red\" = (1,0,0), \"blue\" = (0,1,0), \"green\" = (0,0,1)\n",
    "\n",
    "Pokud se data dělí do jasně odlišných a nezávislých tříd, je *one-hot* kódování výhodné (potřeba) a může významě zlepšit efektivitu neuronového sítě!\n",
    "\n",
    "Specialita:\n",
    "* Metody *word2vec* - pokročilé metody kódování slov na vektorovou reprezentaci"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e1673cd",
   "metadata": {},
   "source": [
    "## Rekurentní neuronové sítě\n",
    "\n",
    "**Motivace** - sekvenční data, obsahující závislosti mezi sousedními prvky\n",
    "\n",
    "**Příklady** - časově závislé sekvence, text\n",
    "\n",
    "*Příklad změny významu v textu:*\n",
    "\n",
    "* The cat chased the mouse.\n",
    "* The mouse chased the cat.\n",
    "\n",
    "Mění se délka sekvence - problém pro konvenční (feed-forward) NN architektury.\n",
    "\n",
    "<img src=\"../images/mlp_on_text.png\" alt=\"Problém s variabilní délkou sekvence pro MLP.\" width=\"1000\"/>\n",
    "\n",
    "**Potřeba zakódovat informaci o pořadí slov (tokenů) přímo v architektuře neuronové sítě.**\n",
    "\n",
    "#### Rekurentní NN:\n",
    "* umožňují zpracovat vstup ve správném pořadí\n",
    "* ukládájí informaci o předcházejících vstupech\n",
    "* umožňují vkládat vstup různých délek\n",
    "* nekteré vstupy/výstupy můžou chybět - překlad řeči, klasifikace, atd..\n",
    "* *Turing complete* = umožňují simulovat libovolný algoritmus! (při poskytnutí dostatku dat..)\n",
    "* *Backpropagation through time* - RNN se učí opakovanou backpropagací v čase\n",
    "\n",
    "<img src=\"../images/rnn2.png\" alt=\"Rekurentní neuronová síť\" width=\"600\"/>\n",
    "\n",
    "Dvě možnosti pro text:\n",
    "* *word-level RNN* - jako token/prvek volíme slovo\n",
    "* *character-level RNN* - jako token/prvek volíme znak\n",
    "\n",
    "#### Word Embedding:\n",
    "* první vrstva přenáší (kóduje) slovo/token do vnitřní reprezentace RNN\n",
    "* po sobě jdoucí slova jsou *zakódovany* aktivací skrytých neuronů\n",
    "\n",
    "#### One-hot encoding:\n",
    "* umožnuje reprezentovat slova/tokeny jako pravděpodobnosti\n",
    "* natrénovanou RNN lze škálovat - ovlivnit \"jak moc si vymýšlí\" nebo jak moc se řídí tím, co se naučila\n",
    "* *softmax* aktivační funkce na výstupu\n",
    "\n",
    "<img src=\"../images/softmax.png\" alt=\"Softmax\" width=\"800\"/>\n",
    "\n",
    "#### Teplota - škálování výstupu\n",
    "* zavedení \"teploty\" a pokročilejšího výběru znaku z pravděpodobností na výstupu\n",
    "* snadný trik - modifikace *softmax aktivační funkce* přidání parametru $T$\n",
    "\n",
    "<img src=\"../images/softmax_temp.png\" alt=\"Softmax\" width=\"300\"/>\n",
    "\n",
    "#### Pokročilé rekurentní NN:\n",
    "* problém s *explodujícími a mizejícími gradienty* - čím je neuronová síť hlubší, tím náročnější je trénování a šíření sekvenční informace\n",
    "* dvě řešení:\n",
    "  - LSTM - *Long Short-Term Memory*\n",
    "  - GRU - *Gated Recurrent Unit*\n",
    "\n",
    "#### LSTM\n",
    "\n",
    "Jednoduché RNN si efektivně pamatují informaci pouze pro krátké řetězce (short-memory). LSTM architektura je vytvořená tak, aby prodloužila tuto \"krátkou pamět\" a umožnila efektivní učení vzájemných i pro dlouhé řetezce slov/tokenů.\n",
    "Je to dosaženo přidáním dalšího *long-term* skrytého stavu $c_t$ spolu s mechanismem pomalého zapomínání a přidávání nové informace.\n",
    "\n",
    "<img src=\"../images/lstm.png\" alt=\"lstm\" width=\"800\"/>\n",
    "\n",
    "#### GRU\n",
    "\n",
    "Podobná architektura jako LSTM, ale jiný mechanismus dlouhodobé paměti.\n",
    "\n",
    "<img src=\"../images/gru.png\" alt=\"gru\" width=\"800\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f4d71af",
   "metadata": {},
   "source": [
    "## ChatGPT a jiní chatboti"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24738700",
   "metadata": {},
   "source": [
    "TODO: stručný uvod/okenko do teorie chatbotu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bde43363",
   "metadata": {},
   "source": [
    "## Zdroje"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7cc7bb1",
   "metadata": {},
   "source": [
    "*Aggarwal - Neural Networks and Deep Learning*\n",
    "\n",
    "https://eclass.upatras.gr/modules/document/file.php/EE935/%CE%97%CE%BB%CE%B5%CE%BA%CF%84%CF%81%CE%BF%CE%BD%CE%B9%CE%BA%CE%AC%20%CE%92%CE%B9%CE%B2%CE%BB%CE%AF%CE%B1/2018_Book_NeuralNetworksAndDeepLearning.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e87527d2",
   "metadata": {},
   "source": [
    "## Kam dál?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f838b33",
   "metadata": {},
   "source": [
    "https://cs.stanford.edu/people/karpathy/convnetjs/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
